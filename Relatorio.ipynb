{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas e dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "# import statsmodels.tsa.api as tsa\n",
    "# from statsmodels.tsa.ar_model import AutoReg\n",
    "# from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# import pmdarima as pm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "#mpl.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "def show_metrics(y_test,prediction, results, name):\n",
    "    print(f'{name} - model Results')\n",
    "    print('r2' , r2_score(prediction, y_test))\n",
    "    print('mse' ,mean_squared_error(prediction, y_test))\n",
    "    print('mae', mean_absolute_error(prediction, y_test))\n",
    "    results[name] = {'r2':r2_score(prediction, y_test), \\\n",
    "                    'mse': mean_squared_error(prediction, y_test), \n",
    "                    'mae': mean_absolute_error(prediction, y_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"chuva_fortaleza.csv\")\n",
    "#df.set_index(df['Year'], inplace=True) \n",
    "#df = df.drop('Year', 1)\n",
    "#df = df['Milimitros']\n",
    "#df.plot()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/datasets/mahirkukreja/delhi-weather-data\n",
    "df = pd.read_csv(\"temp_dehli.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"date\", \"meantemp\"]]\n",
    "df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')\n",
    "df.set_index(df['date'], inplace=True) \n",
    "df = df.drop('date', 1)\n",
    "# df = df['meantemp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando dados em treino, teste e validação. Vale notar que para os modelos lineares utilizaremos apenas treino (75%) e teste (25%), enquanto que para os modelos de machine learning utilizaremos treino (50%), validação (25%) e teste (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:int(len(df) * 0.75)]\n",
    "df_test = df.iloc[int(len(df) * 0.75):]\n",
    "\n",
    "#df_train2 = df.iloc[:int(len(df) * 0.5)]\n",
    "#df_valid2 = df.iloc[int(len(df) * 0.5):int(len(df) * 0.75)]\n",
    "#df_test2 = df.iloc[int(len(df) * 0.75):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 6, 4\n",
    "df_decomp = tsa.seasonal_decompose(df, period=365)\n",
    "df_decomp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estacionariedade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, pvalue, lags, obs, critic, ic = tsa.stattools.adfuller(df)\n",
    "print(pvalue)\n",
    "print(lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferenciação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.diff().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.diff().plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd1 = df.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, pvalue, lags, obs, critic, ic = tsa.stattools.adfuller(dfd1)\n",
    "print(pvalue)\n",
    "print(lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corrwith(df.shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlação com série diferenciada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd1.corrwith(dfd1.shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(dfd1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelação Parcial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlação Parcial com série diferenciada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(dfd1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA (5,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.index = pd.DatetimeIndex(df_train.index.values, freq=df_train.index.inferred_freq)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model = ARIMA(df_train, order=(5,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_arima = arima_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_arima.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_arima.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previsão do treino\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(res_arima.predict(), label='pred')\n",
    "ax.plot(df_train, label='true')\n",
    "plt.title('Dehli - ARIMA (5,1,3) - Treino')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_arima.forecast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model \n",
    "#model = ARIMA(df_train, order=(2, 0, 1))  \n",
    "fitted = res_arima  \n",
    "\n",
    "# Forecast\n",
    "fc = fitted.forecast(394, alpha=0.05)  # 95% conf\n",
    "\n",
    "# Make as pandas series\n",
    "fc_series = pd.Series(fc, index=df_test.index)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,5), dpi=100)\n",
    "plt.plot(df_train, label='treino')\n",
    "plt.plot(df_test, label='test')\n",
    "plt.plot(fc_series, label='predicao')\n",
    "plt.title('Forecast vs Actuals')\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fitted = res_arima  \n",
    "fc = fitted.get_forecast(len(df['meantemp'][int(np.floor((len(df)/100)*75)):]))  \n",
    "conf = fc.conf_int(alpha=0.05) # 95% confidence\n",
    "\n",
    "fc_series = pd.Series(fc.predicted_mean, index=df_test.index)\n",
    "lower_series = pd.Series(conf.iloc[:, 0], index=df_test.index)\n",
    "upper_series = pd.Series(conf.iloc[:, 1], index=df_test.index)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,5), dpi=200)\n",
    "plt.plot(df_train, label='training')\n",
    "plt.plot(df_test, label='actual')\n",
    "plt.plot(fc_series, label='forecast')\n",
    "plt.fill_between(lower_series.index, lower_series, upper_series, \n",
    "                 color='k', alpha=.15)\n",
    "plt.title('Forecast vs Actuals')\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model Results')\n",
    "print('r2' , r2_score(forecast, df_test))\n",
    "print('mse' ,mean_squared_error(forecast, df_test))\n",
    "print('mae', mean_absolute_error(forecast, df_test))\n",
    "results['MA'] = {'r2':r2_score(forecast, df_test), \\\n",
    "                 'mse': mean_squared_error(forecast, df_test), \n",
    "                 'mae': mean_absolute_error(forecast, df_test)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_arima = pm.auto_arima(df_train, max_ar=10, max_ma=5, max_d=2, seasonal=False, trace=True, stepwise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_arima.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_arima.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(auto_arima.predict_in_sample(), label='pred')\n",
    "ax.plot(df_train.values, label='true')\n",
    "plt.title('Dehli - Auto ARIMA MODEL')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(pd.DataFrame(auto_arima.predict(n_periods=394), index=df_test.index), label='pred')\n",
    "#ax.plot(df_train.values, label='true')\n",
    "ax.plot(df_train, label='true')\n",
    "plt.title('Dehli - Auto ARIMA MODEL')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = auto_arima.predict(n_periods=394)\n",
    "\n",
    "print('MA model Results')\n",
    "print('r2' , r2_score(pred, df_test))\n",
    "print('mse' ,mean_squared_error(pred, df_test))\n",
    "print('mae', mean_absolute_error(pred, df_test))\n",
    "results['MA'] = {'r2':r2_score(pred, df_test), \\\n",
    "                 'mse': mean_squared_error(pred, df_test), \n",
    "                 'mae': mean_absolute_error(pred, df_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model \n",
    "model = ARIMA(df_train, order=(2, 0, 1))  \n",
    "fitted = model.fit()  \n",
    "\n",
    "# Forecast\n",
    "fc = fitted.forecast(394, alpha=0.05)  # 95% conf\n",
    "\n",
    "# Make as pandas series\n",
    "fc_series = pd.Series(fc, index=df_test.index)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,5), dpi=100)\n",
    "plt.plot(df_train, label='training')\n",
    "plt.plot(df_test, label='actual')\n",
    "plt.plot(fc_series, label='forecast')\n",
    "plt.title('Forecast vs Actuals')\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = ARIMA(df_train, order=(2, 0, 1))  \n",
    "fitted = model.fit()  \n",
    "\n",
    "fc = fitted.get_forecast(len(df['meantemp'][int(np.floor((len(df)/100)*75)):]))  \n",
    "conf = fc.conf_int(alpha=0.05) # 95% confidence\n",
    "\n",
    "fc_series = pd.Series(fc.predicted_mean, index=df_test.index)\n",
    "lower_series = pd.Series(conf.iloc[:, 0], index=df_test.index)\n",
    "upper_series = pd.Series(conf.iloc[:, 1], index=df_test.index)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,5), dpi=200)\n",
    "plt.plot(df_train, label='training')\n",
    "plt.plot(df_test, label='actual')\n",
    "plt.plot(fc_series, label='forecast')\n",
    "plt.fill_between(lower_series.index, lower_series, upper_series, \n",
    "                 color='k', alpha=.15)\n",
    "plt.title('Forecast vs Actuals')\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time travel\n",
    "def get_lags(series, lags):\n",
    "  result = []\n",
    "  if lags > 0:\n",
    "    for lag in range(1, lags+1):\n",
    "    #  print(lag)\n",
    "    #  print(series.shift(lag))\n",
    "      result.append(series.shift(lag).rename({series.columns[0]: series.columns[0]+'-'+str(lag)}, axis=1))\n",
    "    #return result\n",
    "    return pd.concat(result, axis=1, names=list(range(-1,-lags))).dropna()\n",
    "  else:\n",
    "    for lag in range(-1, lags-1,-1):\n",
    "      #print(lag)\n",
    "      #print(series.shift(lag))\n",
    "      result.append(series.shift(lag).rename({series.columns[0]: series.columns[0]+'+'+str(abs(lag))}, axis=1))\n",
    "    #return result\n",
    "    return pd.concat(result, axis=1, names=list(range(+1,-lags))).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_lags(df, 3)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.reindex(X.index)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separando os dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos o GridSearch para encontrar os melhores parâmetros para o NKK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo hiperpâmetros de busca do GridSearch\n",
    "parameters = {'n_neighbors':range(1,20), 'weights':[\"uniform\", \"distance\"]}\n",
    "for p in parameters.items():\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "knnGS = GridSearchCV(knn, parameters, cv=TimeSeriesSplit(n_splits=10))\n",
    "res = knnGS.fit(X_train, y_train)\n",
    "print(res.best_score_)\n",
    "print(res.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = res.predict(X_test)\n",
    "show_metrics(y_test, prediction, results, 'KNN GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(prediction, label='pred')\n",
    "ax.plot(y_test.reset_index(drop=True), label='true')\n",
    "plt.title('Dehli Temperature - KNN GS MODEL')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuos = prediction.flatten() - y_test.reset_index(drop=True).values.flatten()\n",
    "pd.Series(residuos).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(residuos).plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(residuos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN com série difenciada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diferenciando\n",
    "X_train_d1 = X_train.diff().dropna()\n",
    "X_test_d1 = X_test.diff().dropna()\n",
    "y_train_d1 = y_train.diff().dropna()\n",
    "y_test_d1 = y_test.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_diff = knnGS.fit(X_train_d1, y_train_d1)\n",
    "print(res_diff.best_score_)\n",
    "print(res_diff.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_diff = res_diff.predict(X_test_d1)\n",
    "show_metrics(y_test_d1, prediction_diff, results, 'd1 KNN GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(pd.Series(prediction_diff.flatten()), label='pred')\n",
    "ax.plot(y_test_d1.reset_index(drop=True), label='true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One step ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(pd.Series(prediction_diff.flatten()).cumsum(), label='pred')\n",
    "ax.plot(y_test_d1.reset_index(drop=True).cumsum(), label='true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_one = y_test.shift(1).reset_index(drop=True).add(pd.Series(prediction_diff.flatten(), name='temperature'),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(pred_one, label='pred')\n",
    "ax.plot(y_test.reset_index(drop=True), label='true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_metrics(y_test.iloc[1:-1], pred_one.iloc[1:-1], results, 'pred one KNN GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuos = prediction_diff.flatten() - y_test_d1.reset_index(drop=True).values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(residuos).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(residuos).plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(residuos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "regr = SVR(C=1.0, epsilon=0.2, kernel='linear')\n",
    "\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(regr.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El Nino - SVR model Results')\n",
    "print(r2_score(regr.predict(X_test), y_test))\n",
    "print(mean_squared_error(regr.predict(X_test), y_test))\n",
    "print(mean_absolute_error(regr.predict(X_test), y_test))\n",
    "results['SVR'] = {'r2':r2_score(regr.predict(X_test), y_test), \\\n",
    "                 'mse': mean_squared_error(regr.predict(X_test), y_test), \n",
    "                 'mae': mean_absolute_error(regr.predict(X_test), y_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(regr.predict(X_test), label='pred')\n",
    "ax.plot(y_test.reset_index(drop=True), label='true')\n",
    "plt.title('EL NINO - SVR MODEL')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search (TODO: testar outros parametros) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':['linear','rbf'], 'C':[0.1,1,10]}\n",
    "for p in parameters.items():\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr2 = SVR()\n",
    "regrGS = GridSearchCV(regr2, parameters, cv=TimeSeriesSplit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "res = regrGS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.best_score_)\n",
    "print(res.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVR GS - model Results')\n",
    "print('r2' , r2_score(res.predict(X_test), y_test))\n",
    "print('mse' ,mean_squared_error(res.predict(X_test), y_test))\n",
    "print('mae', mean_absolute_error(res.predict(X_test), y_test))\n",
    "results['SVRGS'] = {'r2':r2_score(res.predict(X_test), y_test), \\\n",
    "                 'mse': mean_squared_error(res.predict(X_test), y_test), \n",
    "                 'mae': mean_absolute_error(res.predict(X_test), y_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(res.predict(X_test), label='pred')\n",
    "ax.plot(y_test.reset_index(drop=True), label='true')\n",
    "plt.title('EL NINO - SVR GS MODEL')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_lags(df, 5)\n",
    "y = df.reindex(X.index)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrco2 = SVR(kernel='linear')\n",
    "regrco2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVR GS - model Results')\n",
    "print('r2' , r2_score(regrco2.predict(X_test), y_test))\n",
    "print('mse' ,mean_squared_error(regrco2.predict(X_test), y_test))\n",
    "print('mae', mean_absolute_error(regrco2.predict(X_test), y_test))\n",
    "results['SVRGS'] = {'r2':r2_score(regrco2.predict(X_test), y_test), \\\n",
    "                 'mse': mean_squared_error(regrco2.predict(X_test), y_test), \n",
    "                 'mae': mean_absolute_error(regrco2.predict(X_test), y_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(regrco2.predict(X_test), label='pred')\n",
    "ax.plot(y_test.reset_index(drop=True), label='true')\n",
    "plt.title('CO2 - SVR GS MODEL')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Verificar se há Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    SVR(kernel='linear'), X, y, train_sizes=[50,100,150,200,250], cv=TimeSeriesSplit())\n",
    "    #SVR(kernel='linear'), X, y, train_sizes=[np.round(np.array(list(range(0.1,1,0.1))) * len(y))], cv=TimeSeriesSplit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(train_scores.mean(axis=1), index=train_sizes).plot(label='train')\n",
    "pd.Series(valid_scores.mean(axis=1), index=train_sizes).plot(label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (TODO: testar arquiteturas e tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divisão de Dados MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separando os dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=.25)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, shuffle=False, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diferenciação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diferenciando\n",
    "X_train_d1 = X_train.diff().dropna()\n",
    "X_valid_d1 = X_valid.diff().dropna()\n",
    "X_test_d1 = X_test.diff().dropna()\n",
    "y_train_d1 = y_train.diff().dropna()\n",
    "y_valid_d1 = y_valid.diff().dropna()\n",
    "y_test_d1 = y_test.diff().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d1_norm = X_train_d1.sub(X_train_d1.mean()).div(X_train_d1.std())\n",
    "X_valid_d1_norm = X_valid_d1.sub(X_valid_d1.mean()).div(X_valid_d1.std())\n",
    "X_test_d1_norm = X_test_d1.sub(X_train_d1.mean()).div(X_train_d1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d1_norm['meantemp-1'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d1_norm['meantemp-1'].plot.hist(title='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_d1_norm['meantemp-1'].plot.hist(title='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_d1_norm['meantemp-1'].plot.hist(title='test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torch.tensor(pd.concat([X_train_d1_norm, y_train_d1], axis=1).values)\n",
    "validating_data = torch.tensor(pd.concat([X_valid_d1_norm, y_valid_d1], axis=1).values)\n",
    "testing_data = torch.tensor(pd.concat([X_test_d1_norm, y_test_d1], axis=1).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=16, shuffle=False)\n",
    "valid_dataloader = DataLoader(validating_data, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(5,100)\n",
    "    self.fc3 = nn.Linear(100,1)\n",
    "    #self.drop1 = nn.Dropout(p=0.5)\n",
    "    #self.fc2 = nn.Linear(64,32)\n",
    "    #self.drop2 = nn.Dropout(p=0.5)\n",
    "  \n",
    "  def forward(self, X):\n",
    "    out = torch.tanh(self.fc1(X))\n",
    "    #out = self.drop1(out)\n",
    "    #out = torch.relu(self.fc2(out))\n",
    "    #out = self.drop2(out)\n",
    "    out = self.fc3(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_neuron = MLP()\n",
    "print(multi_neuron(torch.tensor(X_train_d1.iloc[0]).float()))\n",
    "print(y_train_d1.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_neuron = MLP()\n",
    "epochs = 300\n",
    "loss_fn = nn.MSELoss()\n",
    "#optimizer = optim.RMSprop(multi_neuron.parameters(), lr=0.001)\n",
    "optimizer = optim.SGD(multi_neuron.parameters(), lr=0.01, weight_decay= 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "for epoch in range(1, epochs+1):\n",
    "  loss_train = 0.0\n",
    "  for train_data in train_dataloader:\n",
    "    x = train_data[:,:5].float()\n",
    "    y = train_data[:,5].float()\n",
    "  \n",
    "    #forward pass\n",
    "    outputs = multi_neuron(x)\n",
    "\n",
    "    #loss measure\n",
    "    loss = loss_fn(outputs,y)\n",
    "\n",
    "    #backward pass\n",
    "    optimizer.zero_grad() # pára o autograd\n",
    "    loss.backward() # executa o backpropagation\n",
    "    optimizer.step() # atualiza os pesos\n",
    "\n",
    "    loss_train += loss.item() # soma os erros para obter o erro total\n",
    "\n",
    "  if (epoch % 10 == 0):\n",
    "    print('Epoch{}, loss {}'.format(epoch, loss_train / len(train_dataloader))) # apresenta o erro médio da época\n",
    "  history[epoch] = loss_train / len(train_dataloader)\n",
    "\n",
    "pd.Series(history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_neuron.eval()\n",
    "results_MLP = []\n",
    "for test_data in test_dataloader:\n",
    "    x = test_data[:,:5].float()\n",
    "    y = test_data[:,5].float()\n",
    "    \n",
    "    y_pred = multi_neuron(x)\n",
    "    results.extend(y_pred.flatten().detach().numpy())\n",
    "#pd.DataFrame(results).plot()\n",
    "results_MLP[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.Series(results_MLP, name='pred'), y_test_d1.reset_index(drop=True)],axis=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prices - MLP Results')\n",
    "print('r2' ,r2_score(results_MLP, y_test_d1))\n",
    "print('mse' ,mean_squared_error(results_MLP, y_test_d1))\n",
    "print('mae', mean_absolute_error(results_MLP, y_test_d1))\n",
    "results['MLP'] = {'r2':r2_score(results_MLP, y_test_d1), \\\n",
    "                 'mse': mean_squared_error(results_MLP, y_test_d1), \n",
    "                 'mae': mean_absolute_error(results_MLP, y_test_d1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578d23e9265697bad3ff07bcff4c72684a1087c3ecac6cf42decc0cb53b76ed7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('data_science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
